{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d287bc23-df39-423e-ac02-ab22246980e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import nibabel as nib\n",
    "# import numpy as np\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, average_precision_score\n",
    "# from sklearn.model_selection import train_test_split, KFold\n",
    "# from torch import autocast, GradScaler\n",
    "\n",
    "# # Define the SSM Block for the 3D model\n",
    "# class SSMBlock3D(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(SSMBlock3D, self).__init__()\n",
    "#         # Transition matrix A, input matrix B, and observation matrix C\n",
    "#         self.A = nn.Parameter(torch.randn(hidden_dim, hidden_dim))  # Transition matrix\n",
    "#         self.B = nn.Parameter(torch.randn(input_dim, hidden_dim))    # Input matrix\n",
    "#         self.C = nn.Parameter(torch.randn(hidden_dim, output_dim))   # Observation matrix\n",
    "        \n",
    "#         # Non-linear activation function\n",
    "#         self.activation = nn.GELU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (batch_size, depth, height, width, input_dim)\n",
    "#         B, D, H, W, C = x.shape\n",
    "#         x = x.reshape(B * D * H * W, C)  # Flatten spatial dimensions into one sequence\n",
    "\n",
    "#         # Initialize the hidden state for each sequence\n",
    "#         h = torch.zeros(B * D * H * W, self.B.shape[1]).to(x.device)\n",
    "\n",
    "#         # Apply the input transformation for the entire input tensor\n",
    "#         h = self.activation(torch.matmul(x, self.B))  # Input transformation\n",
    "\n",
    "#         # Update the hidden state with the transition matrix A\n",
    "#         h = self.activation(torch.matmul(h, self.A))\n",
    "\n",
    "#         # Apply the observation matrix C to get the output\n",
    "#         out = torch.matmul(h, self.C)\n",
    "\n",
    "#         # Reshape output back to original spatial dimensions\n",
    "#         out = out.reshape(B, D, H, W, -1)\n",
    "        \n",
    "#         return out\n",
    "\n",
    "\n",
    "# # 3D Vision Mamba Model with SSM blocks and downsampling\n",
    "# class VisionMamba3D(nn.Module):\n",
    "#     def __init__(self, img_size=(240, 240, 155), patch_size=(4, 4, 4), in_chans=1, num_classes=2, embed_dim=96, depths=[4, 4, 4, 4], hidden_dim=128, output_dim=96):\n",
    "#         super(VisionMamba3D, self).__init__()\n",
    "\n",
    "#         self.num_layers = len(depths)\n",
    "#         self.embed_dim = embed_dim\n",
    "\n",
    "#         # Embedding layer (linear projection of patches)\n",
    "#         self.patch_embed = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "#         # SSM layers with downsampling stages\n",
    "#         self.layers = nn.ModuleList()\n",
    "#         self.downsamples = nn.ModuleList()  # Define separate downsample layers\n",
    "\n",
    "#         for i_layer in range(self.num_layers):\n",
    "#             # Add SSM blocks for this stage, pass the correct input_dim (embed_dim)\n",
    "#             stage = nn.Sequential(\n",
    "#                 *[SSMBlock3D(input_dim=self.embed_dim, hidden_dim=hidden_dim, output_dim=self.embed_dim) for _ in range(depths[i_layer])]\n",
    "#             )\n",
    "#             self.layers.append(stage)\n",
    "\n",
    "#             # Add downsampling layers after each stage except the last\n",
    "#             if i_layer < self.num_layers - 1:\n",
    "#                 downsample = nn.Conv3d(self.embed_dim, self.embed_dim * 2, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "#                 self.downsamples.append(downsample)\n",
    "#                 self.embed_dim *= 2  # Update embedding dimension for the next layer\n",
    "\n",
    "#         # Final bottleneck layer\n",
    "#         self.bottleneck = nn.Conv3d(self.embed_dim, self.embed_dim, kernel_size=1)\n",
    "\n",
    "#         # Final MLP classifier based on bottleneck features\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(self.embed_dim, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Patch embedding\n",
    "#         x = self.patch_embed(x)  # Output size should be (B, embed_dim, D/patch, H/patch, W/patch)\n",
    "\n",
    "#         # Reshape to (B, D, H, W, C) format for SSM layers\n",
    "#         B, C, D, H, W = x.shape\n",
    "#         x = x.permute(0, 2, 3, 4, 1)  # Permute to (B, D, H, W, C)\n",
    "\n",
    "#         print(x.shape)\n",
    "#         # SSM blocks with downsampling\n",
    "#         # for i_layer, layer in enumerate(self.layers):\n",
    "#         for i_layer, layer in enumerate(1):\n",
    "#             x = layer(x)\n",
    "#             if i_layer < self.num_layers - 1:\n",
    "#                 x = x.permute(0, 4, 1, 2, 3)  # Permute back to (B, C, D, H, W) for downsampling\n",
    "#                 x = self.downsamples[i_layer](x)  # Apply downsampling\n",
    "#                 x = x.permute(0, 2, 3, 4, 1)  # Permute back to (B, D, H, W, C)\n",
    "\n",
    "#         print(x.shape)\n",
    "#         # Bottleneck feature extraction\n",
    "#         x = x.permute(0, 4, 1, 2, 3)  # Permute to (B, C, D, H, W)\n",
    "#         x = self.bottleneck(x)\n",
    "#         print('train log bottleneck', x)\n",
    "\n",
    "#         # Global average pooling\n",
    "#         x = x.mean(dim=[2, 3, 4])  # Pool over spatial dimensions\n",
    "\n",
    "#         # Classification\n",
    "#         x = self.fc(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# # Dataset for 3D MRI images\n",
    "# class TumorMRIDataset(Dataset):\n",
    "#     def __init__(self, root_dir, limit=None):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.samples = self._load_samples(root_dir, limit)\n",
    "\n",
    "#     def _load_samples(self, root_dir, limit=None):\n",
    "#         samples = []\n",
    "#         for label in ['HGG', 'LGG']:\n",
    "#             label_specific_sample_count = 0\n",
    "#             folder_path = os.path.join(root_dir, label)\n",
    "#             for patient_folder in os.listdir(folder_path):\n",
    "#                 # Find any file that ends with 't1ce.nii'\n",
    "#                 for file_name in os.listdir(os.path.join(folder_path, patient_folder)):\n",
    "#                     if file_name.endswith('t1ce.nii'):\n",
    "#                         img_path = os.path.join(folder_path, patient_folder, file_name)\n",
    "#                         samples.append((img_path, 0 if label == 'HGG' else 1))\n",
    "#                         label_specific_sample_count += 1\n",
    "#                         if limit is not None and label_specific_sample_count >= limit:\n",
    "#                             break\n",
    "#                 if limit is not None and label_specific_sample_count >= limit:\n",
    "#                     break\n",
    "#         return samples\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         file_path, label = self.samples[idx]\n",
    "#         img = nib.load(file_path).get_fdata()\n",
    "#         print(type(img), img.shape)\n",
    "#         img = self._pad_or_crop(img)\n",
    "#         return torch.tensor(img, dtype=torch.float32).permute(2,0,1).unsqueeze(0), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "#     def _pad_or_crop(self, img):\n",
    "#         target_shape = img.shape\n",
    "#         pad_size = [(max(0, target - img_dim)) for target, img_dim in zip(target_shape, img.shape)]\n",
    "#         pad_widths = [(p // 2, p - p // 2) for p in pad_size]\n",
    "#         img_padded = np.pad(img, pad_widths, mode='constant', constant_values=0)\n",
    "#         return img_padded[:target_shape[0], :target_shape[1], :target_shape[2]]\n",
    "\n",
    "\n",
    "# # Split the dataset into train and test sets by class\n",
    "# def split_dataset_by_class(dataset, train_ratio=0.8):\n",
    "#     HGG_samples = [sample for sample in dataset.samples if sample[1] == 0]\n",
    "#     LGG_samples = [sample for sample in dataset.samples if sample[1] == 1]\n",
    "\n",
    "#     # Split each class\n",
    "#     HGG_train, HGG_test = train_test_split(HGG_samples, train_size=train_ratio, shuffle=True)\n",
    "#     LGG_train, LGG_test = train_test_split(LGG_samples, train_size=train_ratio, shuffle=True)\n",
    "\n",
    "#     # Combine the train and test samples\n",
    "#     train_samples = HGG_train + LGG_train\n",
    "#     test_samples = HGG_test + LGG_test\n",
    "\n",
    "#     return train_samples, test_samples\n",
    "\n",
    "\n",
    "# # Training function with mixed precision\n",
    "# def train_model(train_loader, model, criterion, optimizer, scheduler, device, scaler):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Use autocast for mixed precision\n",
    "#         with autocast(device.type):\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Scale the loss before backward pass\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "#         # Now step the scheduler after optimizer step\n",
    "#         scheduler.step()\n",
    "#         running_loss += loss.item()\n",
    "#         print(loss)\n",
    "#         print(loss.item())\n",
    "#         print(running_loss)\n",
    "#     return running_loss / len(train_loader)\n",
    "\n",
    "# # Test and get predictions + loss \n",
    "# def test_model(test_loader, model, criterion, device):\n",
    "#     model.eval()\n",
    "#     pred_labels = []\n",
    "#     running_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             pred_labels.extend(preds.cpu().numpy())\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             running_loss += loss.item()\n",
    "#     return pred_labels, running_loss / len(test_loader)\n",
    "\n",
    "\n",
    "\n",
    "# # Evaluation function\n",
    "# def evaluate_model(test_loader, model, criterion, device):\n",
    "#     model.eval()\n",
    "#     true_labels, pred_labels = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             true_labels.extend(labels.cpu().numpy())\n",
    "#             pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "#     acc = accuracy_score(true_labels, pred_labels)\n",
    "#     cm = confusion_matrix(true_labels, pred_labels)\n",
    "#     f1 = f1_score(true_labels, pred_labels)\n",
    "#     auc = roc_auc_score(true_labels, pred_labels)\n",
    "#     auc_pr = average_precision_score(true_labels, pred_labels)\n",
    "\n",
    "#     return acc, cm, f1, auc, auc_pr\n",
    "\n",
    "\n",
    "# # Cross-validation function\n",
    "# def cross_validate(train_loader, model_class, criterion, optimizer_class, scheduler_class, device, num_epochs=20, k_folds=5):\n",
    "#     kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "#     fold_results = []\n",
    "\n",
    "#     for fold, (train_idx, val_idx) in enumerate(kf.split(train_loader.dataset)):\n",
    "#         print(f\"Fold {fold+1}/{k_folds}\")\n",
    "\n",
    "#         train_subset = torch.utils.data.Subset(train_loader.dataset, train_idx)\n",
    "#         val_subset = torch.utils.data.Subset(train_loader.dataset, val_idx)\n",
    "\n",
    "#         train_loader_fold = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "#         val_loader_fold = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         # Instantiate model and optimizer\n",
    "#         model = model_class().to(device)\n",
    "#         optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=weight_decay)\n",
    "#         scheduler = scheduler_class(optimizer)\n",
    "\n",
    "#         # Initialize GradScaler for mixed precision training\n",
    "#         scaler = GradScaler()\n",
    "\n",
    "#         # Train and evaluate on each fold\n",
    "#         for epoch in range(num_epochs):\n",
    "#             train_loss = train_model(train_loader_fold, model, criterion, optimizer, scheduler, device, scaler)\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "\n",
    "#         val_acc, _, val_f1, val_auc, val_auc_pr = evaluate_model(val_loader_fold, model, criterion, device)\n",
    "#         fold_results.append((val_acc, val_f1, val_auc, val_auc_pr))\n",
    "\n",
    "#     # Return average metrics across folds\n",
    "#     avg_acc = np.mean([r[0] for r in fold_results])\n",
    "#     avg_f1 = np.mean([r[1] for r in fold_results])\n",
    "#     avg_auc = np.mean([r[2] for r in fold_results])\n",
    "#     avg_auc_pr = np.mean([r[3] for r in fold_results])\n",
    "\n",
    "#     return avg_acc, avg_f1, avg_auc, avg_auc_pr\n",
    "\n",
    "\n",
    "# # Results saving function\n",
    "# def save_results_to_file(file_path, cv_acc, test_acc, test_auc, test_auc_pr):\n",
    "#     with open(file_path, 'w') as f:\n",
    "#         f.write(f\"Cross-validation Accuracy: {cv_acc:.4f}\\n\")\n",
    "#         f.write(f\"Test Accuracy: {test_acc:.4f}\\n\")\n",
    "#         f.write(f\"Test AUC: {test_auc:.4f}\\n\")\n",
    "#         f.write(f\"Test AUC-PR: {test_auc_pr:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca3ac538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "VisionMamba3D                            --\n",
       "├─Conv3d: 1-1                            4,704\n",
       "├─ModuleList: 1-2                        --\n",
       "│    └─Sequential: 2-1                   --\n",
       "│    │    └─SSMBlock3D: 3-1              40,960\n",
       "│    │    └─SSMBlock3D: 3-2              40,960\n",
       "│    │    └─SSMBlock3D: 3-3              40,960\n",
       "│    │    └─SSMBlock3D: 3-4              40,960\n",
       "│    └─Sequential: 2-2                   --\n",
       "│    │    └─SSMBlock3D: 3-5              65,536\n",
       "│    │    └─SSMBlock3D: 3-6              65,536\n",
       "│    │    └─SSMBlock3D: 3-7              65,536\n",
       "│    │    └─SSMBlock3D: 3-8              65,536\n",
       "│    └─Sequential: 2-3                   --\n",
       "│    │    └─SSMBlock3D: 3-9              114,688\n",
       "│    │    └─SSMBlock3D: 3-10             114,688\n",
       "│    │    └─SSMBlock3D: 3-11             114,688\n",
       "│    │    └─SSMBlock3D: 3-12             114,688\n",
       "│    └─Sequential: 2-4                   --\n",
       "│    │    └─SSMBlock3D: 3-13             212,992\n",
       "│    │    └─SSMBlock3D: 3-14             212,992\n",
       "│    │    └─SSMBlock3D: 3-15             212,992\n",
       "│    │    └─SSMBlock3D: 3-16             212,992\n",
       "├─ModuleList: 1-3                        --\n",
       "│    └─Conv3d: 2-5                       147,648\n",
       "│    └─Conv3d: 2-6                       590,208\n",
       "│    └─Conv3d: 2-7                       2,360,064\n",
       "├─Conv3d: 1-4                            590,592\n",
       "├─Sequential: 1-5                        --\n",
       "│    └─Linear: 2-8                       393,728\n",
       "│    └─ReLU: 2-9                         --\n",
       "│    └─Linear: 2-10                      1,026\n",
       "=================================================================\n",
       "Total params: 5,824,674\n",
       "Trainable params: 5,824,674\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from model.VisionMamba3D import VisionMamba3D\n",
    "\n",
    "model = VisionMamba3D(\n",
    "    img_size=(155, 240, 240), patch_size=(4, 4, 3), in_chans=1, num_classes=2, embed_dim=96, depths=[4, 4, 4, 4], hidden_dim=128,\n",
    "    ).to('cuda')\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e2eb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "VisionMamba3D                                      --\n",
       "├─Conv3d: 1-1                                      4,704\n",
       "├─ModuleList: 1-2                                  --\n",
       "│    └─Sequential: 2-1                             --\n",
       "│    │    └─TransformerBlockWithSSM: 3-1           111,264\n",
       "│    │    └─TransformerBlockWithSSM: 3-2           111,264\n",
       "│    │    └─TransformerBlockWithSSM: 3-3           111,264\n",
       "│    │    └─TransformerBlockWithSSM: 3-4           111,264\n",
       "│    └─Sequential: 2-2                             --\n",
       "│    │    └─TransformerBlockWithSSM: 3-5           443,712\n",
       "│    │    └─TransformerBlockWithSSM: 3-6           443,712\n",
       "│    │    └─TransformerBlockWithSSM: 3-7           443,712\n",
       "│    │    └─TransformerBlockWithSSM: 3-8           443,712\n",
       "│    └─Sequential: 2-3                             --\n",
       "│    │    └─TransformerBlockWithSSM: 3-9           1,772,160\n",
       "│    │    └─TransformerBlockWithSSM: 3-10          1,772,160\n",
       "│    │    └─TransformerBlockWithSSM: 3-11          1,772,160\n",
       "│    │    └─TransformerBlockWithSSM: 3-12          1,772,160\n",
       "│    └─Sequential: 2-4                             --\n",
       "│    │    └─TransformerBlockWithSSM: 3-13          3,985,344\n",
       "│    │    └─TransformerBlockWithSSM: 3-14          3,985,344\n",
       "│    │    └─TransformerBlockWithSSM: 3-15          3,985,344\n",
       "│    │    └─TransformerBlockWithSSM: 3-16          3,985,344\n",
       "├─ModuleList: 1-3                                  --\n",
       "│    └─Conv3d: 2-5                                 147,648\n",
       "│    └─Conv3d: 2-6                                 590,208\n",
       "│    └─Conv3d: 2-7                                 1,770,048\n",
       "├─Conv3d: 1-4                                      332,352\n",
       "├─Sequential: 1-5                                  --\n",
       "│    └─LayerNorm: 2-8                              1,152\n",
       "│    └─Linear: 2-9                                 295,424\n",
       "│    └─ReLU: 2-10                                  --\n",
       "│    └─Linear: 2-11                                1,026\n",
       "===========================================================================\n",
       "Total params: 28,392,482\n",
       "Trainable params: 28,392,482\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from model.VisionMamba3D_2 import VisionMamba3D\n",
    "\n",
    "model = VisionMamba3D(\n",
    "    img_size=(155, 240, 240), patch_size=(4, 4, 3), in_chans=1, num_classes=2, depths=[4, 4, 4, 4],\n",
    "    ).to('cuda')\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ca614a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66551eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch, length, dim)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Mamba(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# This module uses roughly 3 * expand * d_model^2 parameters\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     d_model\u001b[38;5;241m=\u001b[39mdim, \u001b[38;5;66;03m# Model dimension d_model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,    \u001b[38;5;66;03m# Block expansion factor\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/vmamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vmamba/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vmamba/lib/python3.11/site-packages/mamba_ssm/modules/mamba2_simple.py:138\u001b[0m, in \u001b[0;36mMamba2Simple.forward\u001b[0;34m(self, u, seq_idx)\u001b[0m\n\u001b[1;32m    134\u001b[0m dt_limit_kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt_limit \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(dt_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt_limit)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_mem_eff_path:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# Fully fused path\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmamba_split_conv1d_scan_combined\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzxbcdt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md 1 w -> d w\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrmsnorm_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrmsnorm_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutproj_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutproj_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaddim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaddim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mngroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_before_gate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdt_limit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     z, xBC, dt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(\n\u001b[1;32m    160\u001b[0m         zxbcdt, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_inner, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_inner \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngroups \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/vmamba/lib/python3.11/site-packages/mamba_ssm/ops/triton/ssd_combined.py:908\u001b[0m, in \u001b[0;36mmamba_split_conv1d_scan_combined\u001b[0;34m(zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states, seq_idx, dt_limit, return_final_states, activation, rmsnorm_weight, rmsnorm_eps, outproj_weight, outproj_bias, headdim, ngroups, norm_before_gate)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmamba_split_conv1d_scan_combined\u001b[39m(zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, seq_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dt_limit\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)), return_final_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilu\u001b[39m\u001b[38;5;124m\"\u001b[39m, rmsnorm_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rmsnorm_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, outproj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, outproj_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ngroups\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, norm_before_gate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    890\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    Argument:\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;124;03m        zxbcdt: (batch, seqlen, 2 * dim + 2 * ngroups * dstate + nheads) where dim == nheads * headdim\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;124;03m        out: (batch, seqlen, dim)\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMambaSplitConv1dScanCombinedFn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzxbcdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv1d_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv1d_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_final_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrmsnorm_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrmsnorm_eps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutproj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutproj_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_before_gate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vmamba/lib/python3.11/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/vmamba/lib/python3.11/site-packages/torch/amp/autocast_mode.py:455\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     autocast_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n",
      "File \u001b[0;32m~/miniconda3/envs/vmamba/lib/python3.11/site-packages/mamba_ssm/ops/triton/ssd_combined.py:757\u001b[0m, in \u001b[0;36mMambaSplitConv1dScanCombinedFn.forward\u001b[0;34m(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states, seq_idx, dt_limit, return_final_states, activation, rmsnorm_weight, rmsnorm_eps, outproj_weight, outproj_bias, headdim, ngroups, norm_before_gate)\u001b[0m\n\u001b[1;32m    754\u001b[0m zx0, z, xBC, dt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(zxbcdt, [\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m d_nonssm, dim, dim \u001b[38;5;241m+\u001b[39m ngroups \u001b[38;5;241m*\u001b[39m dstate \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, nheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    755\u001b[0m seq_idx \u001b[38;5;241m=\u001b[39m seq_idx\u001b[38;5;241m.\u001b[39mcontiguous() \u001b[38;5;28;01mif\u001b[39;00m seq_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    756\u001b[0m xBC_conv \u001b[38;5;241m=\u001b[39m rearrange(\n\u001b[0;32m--> 757\u001b[0m     \u001b[43mcausal_conv1d_cuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcausal_conv1d_fwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxBC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb s d -> b d s\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mconv1d_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv1d_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msilu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mswish\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb d s -> b s d\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    760\u001b[0m )\n\u001b[1;32m    761\u001b[0m x, B, C \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(xBC_conv, [dim, ngroups \u001b[38;5;241m*\u001b[39m dstate, ngroups \u001b[38;5;241m*\u001b[39m dstate], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    762\u001b[0m x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb l (h p) -> b l h p\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39mnheads)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from mamba_ssm import Mamba2\n",
    "from mamba_ssm.modules.mamba2_simple import Mamba2Simple as Mamba\n",
    "\n",
    "batch, length, dim = 2, 64, 128\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "model = Mamba(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=dim, # Model dimension d_model\n",
    "    d_state=64,  # SSM state expansion factor, typically 64 or 128\n",
    "    headdim=4,  # Attention head dimension\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n",
    "y = model(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1368fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleS4(nn.Module):\n",
    "    def __init__(self, d_model, seq_len):\n",
    "        super(SimpleS4, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.kernel = nn.Parameter(torch.randn(seq_len))\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, n, d = x.shape\n",
    "        # Convolution with the state-space kernel\n",
    "        x_fft = torch.fft.rfft(x, dim=1)\n",
    "        kernel_fft = torch.fft.rfft(self.kernel, n=n)\n",
    "        kernel_fft = kernel_fft.view(1, -1, 1) \n",
    "        out = torch.fft.irfft(x_fft * kernel_fft, n=n, dim=1)\n",
    "        return self.linear(out)\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Usage\n",
    "seq_len = 102400\n",
    "d_model = 512\n",
    "x = torch.randn(1, seq_len, d_model).to(device)\n",
    "s4_model = SimpleS4(d_model, seq_len).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9609171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = s4_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012c1afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 102400, 512]), torch.Size([1, 102400, 512]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7456bbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct  3 01:16:23 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.61                 Driver Version: 551.61         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   33C    P8             N/A / ERR!  |      46MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2400      C   ...a\\miniconda3\\envs\\vmamba\\python.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b510d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.dataset' from 'c:\\\\Users\\\\shera\\\\Projects\\\\VisionMamba\\\\utils\\\\dataset.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from utils.dataset import TumorMRIDataset\n",
    "import utils\n",
    "reload(utils.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59d55df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import TumorMRIDataset\n",
    "root_dir = './data/MICCAI_BraTS_2019_Data_Training/'\n",
    "# Dataset and DataLoader\n",
    "dataset = TumorMRIDataset(root_dir, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a9340dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, *t2 = dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50a5fe9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, [240, 240, 155])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dcf1990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 50, 120, 120])\n",
      "torch.Size([2, 50, 120, 120, 8])\n",
      "torch.Size([2, 720000, 8])\n",
      "torch.Size([2, 720000, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(2, 8, 50, 120, 120) # (B, C, D, H, W)\n",
    "print(x.shape)\n",
    "x = x.permute(0, 2, 3, 4, 1) # (B, D, H, W, C)\n",
    "print(x.shape)\n",
    "x = x.reshape(x.shape[0], -1, x.shape[-1]) # (B, D*H*W, C)\n",
    "print(x.shape)\n",
    "x = x.reshape(x.shape[0], -1, x.shape[-1]) # (B, D*H*W, C)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a93c06a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from model.modules.ssm import SSM\n",
    "\n",
    "ssm = SSM(\n",
    "    in_features=256,  # Dimension of the transformer model\n",
    "    dt_rank=32,  # Rank of the dynamic routing matrix\n",
    "    dim_inner=256,  # Inner dimension of the transformer model\n",
    "    d_state=256,  # Dimension of the state vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f67771c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model.modules.ssm' from 'c:\\\\Users\\\\shera\\\\Projects\\\\VisionMamba\\\\model\\\\modules\\\\ssm.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import model\n",
    "reload(model.modules.ssm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3eda9283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe1c4da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1728368110.9876351"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57376c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]\n",
    "b = [6,7,8,9,10]\n",
    "\n",
    "torch.save({'a': a, 'b': b}, 'test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edcb1501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shera\\AppData\\Local\\Temp\\ipykernel_15164\\939315028.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x = torch.load('test.pt')\n"
     ]
    }
   ],
   "source": [
    "x = torch.load('test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1cc5dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "008d0ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.randn(2, 51*60*60, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1714c8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[[ 1.292295    0.12974325  1.9748904  ...  0.89918077  0.5026496\\n    0.495291  ]\\n  [-1.1271591   0.507634   -0.22853649 ...  1.1101712  -0.9780393\\n   -1.4631107 ]\\n  [ 0.84693134 -1.4322822   0.93316334 ...  0.7821969  -0.892769\\n   -1.0943215 ]\\n  ...\\n  [-2.1517289   0.88686776  1.237539   ... -1.1798173   0.28577477\\n    1.0158257 ]\\n  [ 0.9999575  -1.5350035  -0.18565762 ... -0.7606606  -0.8681743\\n   -0.50495344]\\n  [-0.9347093  -0.40063527  0.91297567 ...  0.16093272  0.8936355\\n   -0.60487616]]\\n\\n [[ 0.8504815   1.9720266   0.6084179  ... -0.96612144  1.3971791\\n   -0.05779519]\\n  [-0.80218816 -0.95540756  0.46097296 ...  3.0260103  -0.47619528\\n   -2.0453818 ]\\n  [ 1.3117734  -0.79209846  1.7043817  ...  0.2105545  -0.6013392\\n    1.0901706 ]\\n  ...\\n  [-2.1645515   0.10518072  0.4936595  ... -0.74191993  0.59055686\\n   -0.7300158 ]\\n  [-1.4442246  -0.05387694  0.9841744  ...  0.9396119   0.6107726\\n    1.0055447 ]\\n  [ 0.06521791 -0.4809723  -0.06550229 ...  1.2880242   0.04784801\\n    0.66033185]]]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{x.numpy()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64c202ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 183600, 256, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 183600, 256).unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a07db972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 140, Test samples: 36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paths and configurations\n",
    "# root_dir = '/src/workspace/MICCAI_BraTS_2019_Data_Training/'  # Change to your dataset path\n",
    "root_dir = './MICCAI_BraTS_2019_Data_Training/'  # Change to your dataset path\n",
    "batch_size = 2\n",
    "initial_lr = 1e-5  # Lowered learning rate to prevent NaN\n",
    "num_epochs = 20\n",
    "weight_decay = 1e-5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TumorMRIDataset(root_dir, limit=100)\n",
    "train_samples, test_samples = split_dataset_by_class(dataset)\n",
    "print(f\"Train samples: {len(train_samples)}, Test samples: {len(test_samples)}\")\n",
    "\n",
    "# Create datasets and loaders for train and test sets\n",
    "train_dataset = torch.utils.data.Subset(dataset, [dataset.samples.index(s) for s in train_samples])\n",
    "test_dataset = torch.utils.data.Subset(dataset, [dataset.samples.index(s) for s in test_samples])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, Loss, Optimizer, and Scheduler\n",
    "model_class = lambda: VisionMamba3D(img_size=(240, 240, 155), patch_size=(4, 4, 4), in_chans=1, num_classes=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_class = lambda params: optim.AdamW(params, lr=initial_lr, weight_decay=weight_decay)\n",
    "scheduler_class = lambda opt: StepLR(opt, step_size=5, gamma=0.5)\n",
    "\n",
    "# Perform cross-validation\n",
    "# cv_acc, cv_f1, cv_auc, cv_auc_pr = cross_validate(train_loader, model_class, criterion, optimizer_class, scheduler_class, device, num_epochs=num_epochs, k_folds=5)\n",
    "\n",
    "# Train on the full training set and evaluate on the test set\n",
    "model = model_class().to(device)\n",
    "optimizer = optimizer_class(model.parameters())\n",
    "scheduler = scheduler_class(optimizer)\n",
    "\n",
    "# Initialize GradScaler for full training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Train model on the entire training set\n",
    "train_losses, test_losses = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model(train_loader, model, criterion, optimizer, scheduler, device, scaler)\n",
    "    _, test_loss = test_model(test_loader, model, criterion, device)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_acc, _, test_f1, test_auc, test_auc_pr = evaluate_model(test_loader, model, criterion, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
